{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation and Attention\n",
    "In this notebook, we will implement a model for neural machine translation (NMT) with attention. This notebook is adapted from the [TensorFlow tutorial on NMT](https://www.tensorflow.org/tutorials/seq2seq) at  as well as the [TensorFlow NMT package](https://github.com/tensorflow/nmt/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ched/info/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import collections\n",
    "from functools import partial\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Helper TensorFlow functions\n",
    "from utils import maybe_download\n",
    "\n",
    "# The encoder-decoder architecture\n",
    "from nmt.model import AttentionalModel, LSTMCell\n",
    "from nmt.utils import vocab_utils\n",
    "from nmt.train import train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We'll train our model on a small-scale dataset: an English-Vietnamese parallel corpus of TED talks (133K sentence pairs) provided by the IWSLT Evaluation Campaign (https://sites.google.com/site/iwsltevaluation2015/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified datasets/nmt_data_vi/train.en\n",
      "Found and verified datasets/nmt_data_vi/train.vi\n",
      "Found and verified datasets/nmt_data_vi/tst2012.en\n",
      "Found and verified datasets/nmt_data_vi/tst2012.vi\n",
      "Found and verified datasets/nmt_data_vi/tst2013.en\n",
      "Found and verified datasets/nmt_data_vi/tst2013.vi\n",
      "Found and verified datasets/nmt_data_vi/vocab.en\n",
      "Found and verified datasets/nmt_data_vi/vocab.vi\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'vocab.vi'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dir = os.path.join('datasets', 'nmt_data_vi')\n",
    "site_prefix = \"https://nlp.stanford.edu/projects/nmt/data/\"\n",
    "\n",
    "maybe_download(site_prefix + 'iwslt15.en-vi/train.en', out_dir, 13603614)\n",
    "maybe_download(site_prefix + 'iwslt15.en-vi/train.vi', out_dir, 18074646)\n",
    "\n",
    "maybe_download(site_prefix + 'iwslt15.en-vi/tst2012.en', out_dir, 140250)\n",
    "maybe_download(site_prefix + 'iwslt15.en-vi/tst2012.vi', out_dir, 188396)\n",
    "\n",
    "maybe_download(site_prefix + 'iwslt15.en-vi/tst2013.en', out_dir, 132264)\n",
    "maybe_download(site_prefix + 'iwslt15.en-vi/tst2013.vi', out_dir, 183855)\n",
    "\n",
    "maybe_download(site_prefix + 'iwslt15.en-vi/vocab.en', out_dir, 139741)\n",
    "maybe_download(site_prefix + 'iwslt15.en-vi/vocab.vi', out_dir, 46767)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to NMT\n",
    "\n",
    "<figure>\n",
    "    <img src='images/encdec.jpg' alt='missing' />\n",
    "    <figcaption>**Figure 1.** Example of a general, *encoder-decoder* approach to NMT. An encoder converts a source sentence into a representation which is passed through a decoder to produce a translation</figcaption>\n",
    "</figure>\n",
    "\n",
    "A neural machine translation (NMT) system reads in a source sentence using an *encoder*, and then uses a *decoder* to emit a translation. NMT models vary in terms of their exact architectures. A natural choice for sequential data is the recurrent neural network (RNN). Usually an RNN is used for both the encoder and decoder. The RNN models, however, differ in terms of: (a) directionality – unidirectional or bidirectional (whether they read the source sentence in forwards or forwards and backwards); (b) depth – single- or multi-layer; and (c) type – often either a vanilla RNN, a Long Short-term Memory (LSTM), or a gated recurrent unit (GRU).\n",
    "\n",
    "We will consider a deep multi-layer RNN which is bi-directional (it reads the input sequence both forwards and backwards) and uses LSTM units with attention. At a high level, the NMT model consists of two recurrent neural networks: the encoder recurrent network simply consumes the input source words without making any prediction; the decoder, on the other hand, processes the target sentence while predicting the next words.\n",
    "\n",
    "<figure>\n",
    "    <img src='images/seq2seq.jpg' alt='missing' />\n",
    "    <figcaption>**Figure 2.** Example of a neural machine translation system for translating a source sentence \"I am a student\" into a target sentence \"Je suis étudiant\".  Here, $<s>$ marks the start of the decoding process while $</s>$ tells the decoder to stop.\n",
    "    </figcaption>\n",
    "</figure>\n",
    "\n",
    "At the bottom layer, the encoder and decoder recurrent networks receive as input the following: first, the source sentence, then a boundary marker $</s>$ which indicates the transition from the encoding to the decoding mode, and the target sentence. We now go into the details of how the model deals with source and target sentences.\n",
    "\n",
    "### Embedding\n",
    "Given the categorical nature of words, the model must first look up the source and target embeddings to retrieve the corresponding word representations. For this embedding layer to work, a vocabulary is first chosen for each language. Usually, a vocabulary size $V$ is selected, and only the most frequent $V$ words in the corpus are treated as unique. All other words are converted to an \"unknown\" token $<$UNK$>$ and all get the same embedding. The embedding weights, one set per language, are usually learned during training (but pretrained word embeddings may be used instead).\n",
    "\n",
    "### Encoder\n",
    "Once retrieved, the word embeddings are then fed as input into the main network, which consists of two multi-layer recurrent neural networks -- an encoder for the source language and a decoder for the target language. These two networks, in principle, can share the same weights; however, in practice, we often use two different sets of parameters (such models do a better job when fitting large training datasets). The encoder uses zero vectors as its starting states (before it sees the source sequence). In TensorFlow:\n",
    "\n",
    "    # Build RNN cell\n",
    "    encoder_cell = YourEncoderRNNCell(num_units)\n",
    "\n",
    "    # Run Dynamic RNN\n",
    "    #   encoder_outputs: [max_time, batch_size, num_units]\n",
    "    #   encoder_state: [batch_size, num_units]\n",
    "    encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n",
    "        encoder_cell, encoder_emb_inp,\n",
    "        sequence_length=source_sequence_length, time_major=True)\n",
    "\n",
    "### Decoder\n",
    "The decoder also needs to have access to the source information, and one simple way to achieve that is to initialize it with the last hidden state of the encoder, `encoder_state`. In Figure 2, we pass the hidden state at the source word \"student\" to the decoder side.\n",
    "\n",
    "    # Build RNN cell\n",
    "    decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "    \n",
    "    # Helper\n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "        decoder_emb_inp, decoder_lengths, time_major=True)\n",
    "\n",
    "    # Decoder\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        decoder_cell, helper, encoder_state, output_layer=projection_layer)\n",
    "    \n",
    "    # Dynamic decoding\n",
    "    outputs, _ = tf.contrib.seq2seq.dynamic_decode(decoder, ...)\n",
    "    logits = outputs.rnn_output\n",
    "\n",
    "### Loss\n",
    "Given the logits above, we are now ready to compute the training loss:\n",
    "\n",
    "    xent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=decoder_outputs, logits=logits)\n",
    "    train_loss = (tf.reduce_sum(crossent * target_weights) / batch_size)\n",
    "\n",
    "Here, target_weights is a zero-one matrix of the same size as decoder_outputs. It masks padding positions outside of the target sequence lengths with values 0.\n",
    "\n",
    "Important note: It's worth pointing out that we should divide the loss by `batch_size`, so our hyperparameters are \"invariant\" to `batch_size`. Some people divide the loss by (`batch_size * num_time_steps`), which plays down the errors made on short sentences. More subtly, the same hyperparameters (applied to the former way) can't be used for the latter way. For example, if both approaches use SGD with a learning of `1.0`, the latter approach effectively uses a much smaller learning rate of `1 / num_time_steps`.\n",
    "\n",
    "### How to generate translations at test time\n",
    "\n",
    "While you're training your NMT models (and once you have trained models), you can obtain translations given previously unseen source sentences. At test time, we only have access to the source sentence; i.e., `encoder_inputs`. There are many ways to perform decoding given those inputs. Decoding methods include greedy, sampling, and beam-search decoding. Here, we will discuss the greedy decoding strategy.\n",
    "\n",
    "The idea is simple and illustrated in Figure 3:\n",
    "\n",
    "1. We still encode the source sentence in the same way as during training to obtain an `encoder_state`, and this `encoder_state` is used to initialize the decoder.\n",
    "\n",
    "2. The decoding (translation) process is started as soon as the decoder receives a starting symbol $<$/s$>$.\n",
    "\n",
    "3. For each timestep on the decoder side, we treat the recurrent network's output as a set of logits. We choose the most likely word, the id associated with the maximum logit value, as the emitted word (this is the \"greedy\" behavior). For example in Figure 3, the word \"moi\" has the highest translation probability in the first decoding step. We then feed this word as input to the next timestep. (At training time, however, we may feed in the true target as input to the next timestep in a process called *teacher forcing*.)\n",
    "\n",
    "4. The process continues until the end-of-sentence marker $<$/s$>$ is produced as an output symbol.\n",
    "\n",
    "<figure>\n",
    "    <img src='images/greedy_dec.jpg' alt='missing' />\n",
    "    <figcaption>**Figure 3.** Example of how a trained NMT model produces a translation for a source sentence \"Je suis étudiant\" using greedy search.\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Attention\n",
    "\n",
    "The attention mechanism was first introduced by Bahdanau et al., 2015 [1] and then later refined by Luong et al., 2015 [2] and others. The key idea of the attention mechanism is to establish direct short-cut connections between the target and the source by paying \"attention\" to relevant source content as we translate (produce output tokens). A nice byproduct of the attention mechanism is an easy-to-visualize alignment matrix between the source and target sentences that we will visualize at the end of this notebook.\n",
    " \n",
    "Remember that in a vanilla seq2seq model, we pass the last source state $h_{s_{T_s}}$ from the encoder to the decoder when starting the decoding process. This works well for short and medium-length sentences; however, for long sentences, the single fixed-size hidden state becomes an information bottleneck. Instead of discarding all of the hidden states computed in the source RNN, the attention mechanism provides an approach that allows the decoder to peek at them (treating them as a dynamic memory of the source information). By doing so, the attention mechanism improves the translation of longer sentences. Nowadays, attention mechanisms are the *de facto* standard and have been successfully applied to many other tasks (including image caption generation, speech recognition, and text summarization).\n",
    "\n",
    "<figure>\n",
    "    <img src='images/att.jpg' alt='missing' />\n",
    "    <figcaption>**Figure 4.** Example of an attention-based NMT system with the first step of the attention computation in detail. For clarity, the embedding and projection layers are omitted.\n",
    "    </figcaption>\n",
    "</figure>\n",
    "\n",
    "### How do we actually attend over the input sequence?\n",
    "\n",
    "There are many different ways of formalizing attention. These variants depend on the form of a *scoring* function and an *attention* function (and on whether the previous state of the decoder $h_{t_{i-1}}$ is used instead of $h_{t_{i}}$ in the scoring function as originally suggested in Bahdanau et al. (2015); **we will stick to using $h_{t_{i}}$** in this notebook). Luong et al. (2015) demonstrate that only a few choices actually matter:\n",
    "\n",
    "1. First, the basic form of attention, i.e., **direct connections between target and source**, needs to be present. \n",
    "\n",
    "2. Second, it's important to **feed the attention vector to the next timestep** to inform the network about past attention decisions.\n",
    "\n",
    "3. Lastly, **choices of the scoring function** can often result in different performance. See Luong et al. (2015) for further details.\n",
    "\n",
    "### A general framework for computing attention\n",
    "\n",
    "The attention computation happens at every decoder time step. It consists of the following stages:\n",
    "\n",
    "1. The current target (encoder) hidden state $h_{t_i}$ is compared with all source (decoder) states $h_{s_j}$ to derive *attention weights* $\\alpha_{ij}$.\n",
    "2. Based on the attention weights we compute a *context vector* $c_{i}$ as the weighted average of the source states.\n",
    "3. We combine the context vector $c_{i}$ with the current target hidden state $h_{s_j}$ to yield the final *attention vector* $a_t$.\n",
    "4. The attention vector $a_i$ is fed as an input to the next time step (*input feeding*). \n",
    "\n",
    "The first three steps can be summarized by the equations below:\n",
    "\n",
    "$$\\large\\begin{align*}\n",
    "\\alpha_{ij} &= \\frac{\n",
    "    \\exp(\\text{score}(h_{t_i}, h_{s_j}))\n",
    "}{\n",
    "    \\sum_{k=1}^{T_s}{\\exp(\\text{score}(h_{t_i}, h_{s_k}))}\n",
    "} \\tag{attention weights} \\\\\\\\\n",
    "c_{i} &= \\sum_{j=1}^{T_s} \\alpha_{ij} h_{s_j} \\tag{context vector} \\\\\\\\\n",
    "a_{i} &= f(c_{i}, h_{t_i}) \\tag{attention vector} \\\\\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Here, the function `score` is used to compare the target hidden state $h_{t_i}$ with each of the source hidden states $h_{s_j}$, and the result is normalized over the source timesteps $j = 1, \\dots, T_s$ to produce attention weights $\\alpha_{ij}$ (which define a distribution over source positions $j$ for a given source timestep $i$). (There are various choices of the scoring function; we will consider three below.) Note that we make use of the current decoder (or *target*) hidden state $h_{t_i}$, which is computed as a function of the previous hidden state $h_{t_{i-1}}$, the embedding of the input token $x_{i}$ (which is either the emission or the ground truth token from the previous timestep) using the standard formula for a recurrent cell. Optionally, in the case of *input feeding*, we combine $h_{t_{i-1}}$ with the context vector from the previous timestep, $c_{t_{i-1}}$ (which may require a change in the size of the kernel matrix, depending on how the combination is implemented). The encoder (or *source*) hidden states $h_{s_j}$ for $j=1, \\dots T_s$ are similarly the standard hidden state for a recurrent cell.\n",
    "\n",
    "We can also vectorize the computation of the context vector $c_i$ for every target timestep as follows: Given the source hidden states $h_{s_1}, \\dots, h_{s_{T_s}}$, we construct a matrix $H_s$ of size `hidden_size` $\\times$ `input_seq_len` by stacking the source hidden states into columns. Attention allows us to dynamically weight certain timesteps of the input sequence in a fixed size vector $c_i$ by taking a convex combination of the columns of $H_s$. In particular, we calculate a nonzero and normalized attention weight vector $\\vec{\\alpha}_i = [\\alpha_{i1}, \\dots, \\alpha_{iT_s}]^T$ that weights the source hidden states in the computation\n",
    "\n",
    "$$\\large c_i = H_s\\vec{\\alpha}_i~.$$\n",
    "\n",
    "\n",
    "\n",
    "The attention vector $a_i$ is used to derive the softmax logits and thereafter the loss by transformation under a function $f$.The function $f$ is commonly the a concatenation followed by $\\tanh$ layer:\n",
    "\n",
    "$$\\large a_{i} = \\tanh(W_a[c_i; h_{t_i}])$$\n",
    "\n",
    "but could take other forms. We then compute the predictive distribution over output tokens as\n",
    "\n",
    "$$\\large p(y_i \\mid y_1, \\dots y_{i-1}, x_i) = \\text{softmax}(W_s a_{i})~.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. LSTM cell with attention (8 pts)\n",
    "\n",
    "In the block below, you will implement the method `call`, which computes a single step of an LSTM cell using a method `attention` that computes an attention vector with some score function, as described above. **Complete the skeleton below**; assume inputs is already the input embedding (i.e., there is no need to construct an embedding matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCellWithAttention(LSTMCell):\n",
    "    \n",
    "    def __init__(self, num_units, memory):\n",
    "        print(num_units)\n",
    "        super(LSTMCellWithAttention, self).__init__(num_units)\n",
    "        self.memory = memory\n",
    "        \n",
    "    def attention(self):\n",
    "        raise NotImplementedError(\"The subclass must implement this method!\")\n",
    "\n",
    "    def call(self, inputs, state):\n",
    "        \"\"\"Run this LSTM cell with attention on inputs, conditional on state.\"\"\"\n",
    "        \n",
    "        # Cell and hidden states of the LSTM\n",
    "        c, h = state\n",
    "        \n",
    "        # Source (encoder) states to attend over\n",
    "        source_states = self.memory\n",
    "        \n",
    "        # Cell activation (e.g., tanh, relu, etc.)\n",
    "        activation = self._activation\n",
    "        \n",
    "        # LSTM cell parameters\n",
    "        kernel = self._kernel\n",
    "        bias = self._bias\n",
    "        forget_bias = self._forget_bias\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        from tensorflow.python.ops import math_ops\n",
    "        from tensorflow.python.framework import constant_op\n",
    "        from tensorflow.python.ops import array_ops\n",
    "        from tensorflow.python.ops import nn_ops\n",
    "        sigmoid = math_ops.sigmoid\n",
    "        one = constant_op.constant(1, dtype=tf.int32)\n",
    "        attention_vector = self.attention(h,source_states)\n",
    "        gate_inputs = math_ops.matmul(\n",
    "                    array_ops.concat([inputs, h], 1), self._kernel)\n",
    "        gate_inputs = nn_ops.bias_add(gate_inputs, self._bias)\n",
    "        \n",
    "        # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
    "        i, j, f, o = array_ops.split(\n",
    "        value=gate_inputs, num_or_size_splits=4, axis=one)\n",
    "\n",
    "        forget_bias_tensor = constant_op.constant(self._forget_bias, dtype=f.dtype)\n",
    "        # Note that using `add` and `multiply` instead of `+` and `*` gives a\n",
    "        # performance improvement. So using those at the cost of readability.\n",
    "        add = math_ops.add\n",
    "        multiply = math_ops.multiply\n",
    "        new_c = add(multiply(c, sigmoid(add(f, forget_bias_tensor))),\n",
    "                    multiply(sigmoid(i), self._activation(j)))\n",
    "        new_h = multiply(self._activation(new_c), sigmoid(o))\n",
    "        \n",
    "      \n",
    "        \n",
    "        ### END YOUR CODE\n",
    "        ### Your code should compute attention vector, new_c and new_h\n",
    "\n",
    "        # Adhering to convention\n",
    "        new_state = tf.contrib.rnn.LSTMStateTuple(new_c, new_h)\n",
    "    \n",
    "        return attention_vector, new_state "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement a \"dummy\" version of attention in order to test that the LSTM cell step function is working correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCellWithDummyAttention(LSTMCellWithAttention):\n",
    "\n",
    "    def attention(self, target_state, source_states):\n",
    "        \"\"\"Just return the target state so that the update becomes the vanilla\n",
    "        LSTM update.\"\"\"\n",
    "        \n",
    "        return target_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2A. Dot-product Attention (8 pts)\n",
    "\n",
    "We first consider the simplest version of attention, which simply calculates the similarity between $h_{t_i}$ and $h_{s_j}$ by computing their dot product:\n",
    "\n",
    "$$\\large\\begin{align*}\n",
    "\\text{score}(h_{t_i}, h_{s_j})&=h_{t_i}^\\mathrm{\\,T}\\, h_{s_j}~.\n",
    "\\end{align*}$$\n",
    "\n",
    "This computation has no additional parameters, but it limits the expressivity of the model since its forces the input and output encodings to be close in order to have high score.\n",
    "\n",
    "For this question, **implement the __call__ function of the following LSTM cell using dot-product attention.** Your code should be less than ten lines and *not* make use of any higher-level primitives from `tf.nn` or `tf.layers`, etc. (6 pts). As a further step, **vectorize the operation** so that you can compute $\\text{score}(\\cdot, h_{s_j})$ for every word in the source sentence in parallel (2 pts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCellWithDotProductAttention(LSTMCellWithAttention):\n",
    "        \n",
    "    def build(self, inputs_shape):\n",
    "        super(LSTMCellWithDotProductAttention, self).build(inputs_shape)\n",
    "        self._W_c = self.add_variable(\"W_c\", \n",
    "                                      shape=[self._num_units + self._num_units, \n",
    "                                             256])\n",
    "\n",
    "    def attention(self, target_state, source_states):\n",
    "        \"\"\"Return the attention vector computed from attending over\n",
    "        source_states using a function of target_state and source_states.\"\"\"\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        \n",
    "        tmp_target_state = tf.expand_dims(target_state,axis=1)\n",
    "        dot_prod =  tf.reduce_sum(tf.multiply(source_states,tmp_target_state), 2,keep_dims=True)\n",
    "        _max = tf.reduce_max(dot_prod,axis=2,keep_dims=True)\n",
    "        dot_prod = tf.subtract(dot_prod,_max)\n",
    "        exp_scores = tf.exp(dot_prod)\n",
    "        _sum = tf.reduce_sum(exp_scores,1)\n",
    "        tmp_sum = 1.0/tf.expand_dims(_sum,axis=1)\n",
    "        alpha = tf.multiply(exp_scores,tmp_sum)\n",
    "        alpha = tf.tile(alpha,(1,1,target_state.get_shape()[1]))   \n",
    "        c =  tf.reduce_sum(tf.multiply(alpha,source_states),axis=1,keep_dims=True)\n",
    "        c = tf.squeeze(c,[1])\n",
    "        \n",
    "       \n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        ### Your code should compute the context vector c\n",
    "    \n",
    "      \n",
    "        attention_vector = tf.tanh(tf.matmul(tf.concat([c, target_state], -1), self._W_c))\n",
    "        \n",
    "       \n",
    "        return attention_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2B. Bilinear Attention (8 pts)\n",
    "\n",
    "To make the score function more expressive, we may consider using a bilinear function of the form\n",
    "\n",
    "$$\\large\\begin{align*}\n",
    "\\text{score}(h_{t_i}, h_{s_j})&=h_{t_i}^\\mathrm{\\,T} W_\\text{att} h_{s_j}~,\n",
    "\\end{align*}$$\n",
    "\n",
    "which transforms the source encoding $h_{s_j}$ by a linear transformation parameterized by $W_\\text{att}$ before taking the dot product. This formulation adds additional parameters that must be learned, but increases expressivity and also allows the source and target encodings to be of different dimensionality (if we so wish).\n",
    "\n",
    "For this question, **implement the __call__ function of the following LSTM cell using bilinear attention.** Your code should be less than ten lines and *not* make use of any higher-level primitives from `tf.nn`or `tf.layers`, etc. (6 pts). As a further step, **vectorize the operation** so that you can compute $\\text{score}(\\cdot, h_{s_j})$ for every word in the source sentence in parallel (2 pts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCellWithBilinearAttention(LSTMCellWithAttention):\n",
    "    \n",
    "    def build(self, inputs_shape):\n",
    "        super(LSTMCellWithBilinearAttention, self).build(inputs_shape)\n",
    "        self._W_att = self.add_variable(\"W_att\", \n",
    "                                        shape=[self._num_units, \n",
    "                                               self._num_units])\n",
    "        self._W_c = self.add_variable(\"W_c\", \n",
    "                                      shape=[self._num_units + self._num_units, \n",
    "                                             256])\n",
    "\n",
    "    def attention(self, target_state, source_states):\n",
    "        \"\"\"Return the attention vector computed from attending over\n",
    "        source_states using a function of target_state and source_states.\"\"\"\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        \n",
    "        att_vec = tf.matmul( target_state, self._W_att )\n",
    "        tmp_att_vec = tf.expand_dims(att_vec,axis=1)\n",
    "        dot_prod = tf.reduce_sum(tf.multiply(tmp_att_vec,source_states), axis=2,keep_dims=True)\n",
    "        _max = tf.reduce_max(dot_prod,axis=2,keep_dims=True)\n",
    "        dot_prod = tf.subtract(dot_prod,_max)\n",
    "        exp_scores = tf.exp(dot_prod)\n",
    "        _sum = tf.reduce_sum(exp_scores,1)\n",
    "        tmp_sum = 1.0/tf.expand_dims(_sum,axis=1)\n",
    "        alpha = tf.multiply(exp_scores,tmp_sum)\n",
    "        alpha = tf.tile(alpha,(1,1,target_state.get_shape()[1]))   \n",
    "        c =  tf.reduce_sum(tf.multiply(alpha,source_states),axis=1,keep_dims=True)\n",
    "        c = tf.squeeze(c,[1])\n",
    "      \n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        ### Your code should compute the context vector c\n",
    "        attention_vector = tf.tanh(tf.matmul(tf.concat([c, target_state], -1), self._W_c))\n",
    "       \n",
    "        \n",
    "        return attention_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2C. Feedforward Attention (8 pts)\n",
    "\n",
    "Instead of simply using a linear transformation, why don't we use an even more expressive feedforward neural network to compute the score?\n",
    "\n",
    "$$\\large\\begin{align*}\n",
    "\\text{score}(h_{t_i}, h_{s_j})&=W_{\\text{att}_2} \\tanh( W_{\\text{att}_1} [h_{t_i}; h_{s_j}])~,\n",
    "\\end{align*}$$\n",
    "\n",
    "where $[v_1; v_2]$ denotes a concatenation of the vectors $v_1$ and $v_2$, and $W_{\\text{att}_1}$ and $W_{\\text{att}_2}$ are learned parameter matrices. The feedforward approach typically has fewer parameters (depending on the size of the hidden layer) than the bilinear attention mechanism (which requires `source_embedding_dim` $\\times$ `target_embedding_dim` parameters).\n",
    "\n",
    "For this question, **implement the __call__ function of the following LSTM cell using feedforward attention.** Your code should be less than ten lines and *not* make use of any higher-level primitives from `tf.nn` or `tf.layers`, etc. (6 pts). As a further step, **vectorize the operation** so that you can compute $\\text{score}(\\cdot, h_{s_j})$ for every word in the source sentence in parallel (2 pts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCellWithFeedForwardAttention(LSTMCellWithAttention):\n",
    "    \n",
    "    def build(self, inputs_shape):\n",
    "        super(LSTMCellWithFeedForwardAttention, self).build(inputs_shape)\n",
    "\n",
    "        self._W_att_1 = self.add_variable(\"W_att_1\", \n",
    "                                          shape=[self._num_units + self._num_units, \n",
    "                                                 self._num_units])\n",
    "        self._W_att_2 = self.add_variable(\"W_att_2\", \n",
    "                                          shape=[self._num_units, 1])\n",
    "        self._W_c = self.add_variable(\"W_c\", \n",
    "                                      shape=[self._num_units + self._num_units, \n",
    "                                             256])\n",
    "        \n",
    "    def attention(self, target_state, source_states):\n",
    "        \"\"\"Return the attention vector computed from attending over\n",
    "        source_states using a function of target_state and source_states.\"\"\"\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        sources_length = tf.shape(source_states)[1]\n",
    "        tmp_target_state = tf.expand_dims(target_state,axis=1)\n",
    "        target_repeated = tf.tile(tmp_target_state, (1,sources_length, 1))\n",
    "        tmp_1 = tf.concat([target_repeated, source_states], -1)\n",
    "        tmp_2 = tf.tanh(tf.tensordot(tmp_1,self._W_att_1,[[2],[0]]))\n",
    "        score = tf.tensordot(tmp_2,tf.transpose(self._W_att_2),[[2],[1]])\n",
    "        _max = tf.reduce_max(score,axis=2,keep_dims=True)\n",
    "        dot_prod = tf.subtract(score,_max)\n",
    "        exp_scores = tf.exp(score)\n",
    "        _sum = tf.reduce_sum(exp_scores, 1,keep_dims=True)\n",
    "        alpha = tf.divide(exp_scores,_sum)\n",
    "        c =  tf.reduce_sum(tf.multiply(alpha,source_states),axis=1,keep_dims=True)\n",
    "        c = tf.squeeze(c,[1])\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        ### Your code should compute the context vector c\n",
    "        attention_vector = tf.tanh(tf.matmul(tf.concat([c, target_state], -1), self._W_c))\n",
    "        \n",
    "        return attention_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter settings\n",
    "\n",
    "You may find it useful to tune some of these parameters (but not necessarily)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_standard_hparams(data_path, out_dir):\n",
    "    \n",
    "    hparams = tf.contrib.training.HParams(\n",
    "        \n",
    "        # Data\n",
    "        src=\"vi\",\n",
    "        tgt=\"en\",\n",
    "        train_prefix=os.path.join(data_path, \"train\"),\n",
    "        dev_prefix=os.path.join(data_path, \"tst2012\"),\n",
    "        test_prefix=os.path.join(data_path, \"tst2013\"),\n",
    "        vocab_prefix=\"\",\n",
    "        embed_prefix=\"\",\n",
    "        out_dir=out_dir,\n",
    "        src_vocab_file=os.path.join(data_path, \"vocab.vi\"),\n",
    "        tgt_vocab_file=os.path.join(data_path, \"vocab.en\"),\n",
    "        src_embed_file=\"\",\n",
    "        tgt_embed_file=\"\",\n",
    "        src_file=os.path.join(data_path, \"train.vi\"),\n",
    "        tgt_file=os.path.join(data_path, \"train.en\"),\n",
    "        dev_src_file=os.path.join(data_path, \"tst2012.vi\"),\n",
    "        dev_tgt_file=os.path.join(data_path, \"tst2012.en\"),\n",
    "        test_src_file=os.path.join(data_path, \"tst2013.vi\"),\n",
    "        test_tgt_file=os.path.join(data_path, \"tst2013.en\"),\n",
    "\n",
    "        # Networks\n",
    "        num_units=512,\n",
    "        num_layers=1,\n",
    "        num_encoder_layers=1,\n",
    "        num_decoder_layers=1,\n",
    "        num_encoder_residual_layers=0,\n",
    "        num_decoder_residual_layers=0,\n",
    "        dropout=0.2,\n",
    "        unit_type=\"lstm\",\n",
    "        encoder_type=\"uni\",\n",
    "        residual=False,\n",
    "        time_major=True,\n",
    "        num_embeddings_partitions=0,\n",
    "\n",
    "        # Train\n",
    "        optimizer=\"adam\",\n",
    "        batch_size=128,\n",
    "        init_op=\"uniform\",\n",
    "        init_weight=0.1,\n",
    "        max_gradient_norm=100.0,\n",
    "        learning_rate=0.001,\n",
    "        warmup_steps=0,\n",
    "        warmup_scheme=\"t2t\",\n",
    "        decay_scheme=\"luong234\",\n",
    "        colocate_gradients_with_ops=True,\n",
    "        num_train_steps=1200,\n",
    "\n",
    "        # Data constraints\n",
    "        num_buckets=5,\n",
    "        max_train=0,\n",
    "        src_max_len=25,\n",
    "        tgt_max_len=25,\n",
    "        src_max_len_infer=0,\n",
    "        tgt_max_len_infer=0,\n",
    "\n",
    "        # Data format\n",
    "        sos=\"<s>\",\n",
    "        eos=\"</s>\",\n",
    "        subword_option=\"\",\n",
    "        check_special_token=True,\n",
    "\n",
    "        # Misc\n",
    "        forget_bias=1.0,\n",
    "        num_gpus=1,\n",
    "        epoch_step=0,  # record where we were within an epoch.\n",
    "        steps_per_stats=100,\n",
    "        steps_per_external_eval=0,\n",
    "        share_vocab=False,\n",
    "        metrics=[\"bleu\"],\n",
    "        log_device_placement=False,\n",
    "        random_seed=None,\n",
    "        # only enable beam search during inference when beam_width > 0.\n",
    "        beam_width=0,\n",
    "        length_penalty_weight=0.0,\n",
    "        override_loaded_hparams=True,\n",
    "        num_keep_ckpts=5,\n",
    "        avg_ckpts=False,\n",
    "        num_intra_threads=0,\n",
    "        num_inter_threads=0,\n",
    "\n",
    "        # For inference\n",
    "        inference_indices=None,\n",
    "        infer_batch_size=32,\n",
    "        sampling_temperature=0.0,\n",
    "        num_translations_per_input=1,\n",
    "        \n",
    "    )\n",
    "    \n",
    "    src_vocab_size, _ = vocab_utils.check_vocab(hparams.src_vocab_file, hparams.out_dir)\n",
    "    tgt_vocab_size, _ = vocab_utils.check_vocab(hparams.tgt_vocab_file, hparams.out_dir)\n",
    "    hparams.add_hparam('src_vocab_size', src_vocab_size)\n",
    "    hparams.add_hparam('tgt_vocab_size', tgt_vocab_size)\n",
    "    \n",
    "    out_dir = hparams.out_dir\n",
    "    if not tf.gfile.Exists(out_dir):\n",
    "        tf.gfile.MakeDirs(out_dir)\n",
    "         \n",
    "    for metric in hparams.metrics:\n",
    "        hparams.add_hparam(\"best_\" + metric, 0)  # larger is better\n",
    "        best_metric_dir = os.path.join(hparams.out_dir, \"best_\" + metric)\n",
    "        hparams.add_hparam(\"best_\" + metric + \"_dir\", best_metric_dir)\n",
    "        tf.gfile.MakeDirs(best_metric_dir)\n",
    "\n",
    "        if hparams.avg_ckpts:\n",
    "            hparams.add_hparam(\"avg_best_\" + metric, 0)  # larger is better\n",
    "            best_metric_dir = os.path.join(hparams.out_dir, \"avg_best_\" + metric)\n",
    "            hparams.add_hparam(\"avg_best_\" + metric + \"_dir\", best_metric_dir)\n",
    "            tf.gfile.MakeDirs(best_metric_dir)\n",
    "\n",
    "    return hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Training (8 pts)\n",
    "\n",
    "For this question, **train at least two of the models that use the attention modules you defined above**. Did you notice any difference in the training or evaluation of the different models? **Provide a brief written answer below.**\n",
    "\n",
    "*Note*: Make sure you **remove the model checkpoints** in the appropriate folders (`nmt_model_dotprod_att`, `nmt_model_binlinear_att` or `nmt_model_feedforward_att`)  if you would like to start training from scratch. (It's safe to delete all the files saved in the directory, or move them elsewhere.) Otherwise, the saved parameters will automatically be reloaded from the latest checkpoint and training will resume where it left off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your written answer here!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Vocab file datasets/nmt_data_vi/vocab.vi exists\n",
      "# Vocab file datasets/nmt_data_vi/vocab.en exists\n",
      "# creating train graph ...\n",
      "  num_layers = 1, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
      "  cell 0512\n",
      "  DropoutWrapper, dropout=0.2   DropoutWrapper  DeviceWrapper, device=/gpu:0\n",
      "  learning_rate=0.001, warmup_steps=0, warmup_scheme=t2t\n",
      "  decay_scheme=luong234, start_decay_step=800, decay_steps 100, decay_factor 0.5\n",
      "# Trainable variables\n",
      "  embeddings/encoder/embedding_encoder:0, (7709, 512), /device:GPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (17191, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_dummy_attention/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_dummy_attention/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (512, 17191), \n",
      "# creating eval graph ...\n",
      "  num_layers = 1, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 0512\n",
      "  LSTMCellWithDummyAttention, dropout=0   LSTMCellWithDummyAttention  DeviceWrapper, device=/gpu:0\n",
      "# Trainable variables\n",
      "  embeddings/encoder/embedding_encoder:0, (7709, 512), /device:GPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (17191, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_dummy_attention/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_dummy_attention/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (512, 17191), \n",
      "# creating infer graph ...\n",
      "  num_layers = 1, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 0512\n",
      "  LSTMCellWithDummyAttention, dropout=0   LSTMCellWithDummyAttention  DeviceWrapper, device=/gpu:0\n",
      "# Trainable variables\n",
      "  embeddings/encoder/embedding_encoder:0, (7709, 512), /device:GPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (17191, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_dummy_attention/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_dummy_attention/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (512, 17191), \n",
      "  created train model with fresh parameters, time 1.55s\n",
      "  created infer model with fresh parameters, time 0.45s\n",
      "  # 967\n",
      "    src: Và thế là -- Nó nói , &quot; Quyển sách này là của ... &quot; Và thế là tôi gõ tên tôi vào .\n",
      "    ref: And so -- It says , &quot; This box belongs to ... &quot; And so I type in my name .\n",
      "    nmt: aims aims aims Association Association Minority Minority Minority island island island strands strands agency agency eventually eventually eventually incubator incubator incubator broccoli broccoli broccoli broccoli Zambia Zambia Zambia repeated repeated repeated repeated repeated repeated repeated repeated repeated repeated backstage remember remember remember remember remember remember remember remember multiplayer\n",
      "  created eval model with fresh parameters, time 0.56s\n",
      "  eval dev: perplexity 17306.20, time 43s, Tue Apr  3 22:11:25 2018.\n",
      "  eval test: perplexity 17381.93, time 41s, Tue Apr  3 22:12:07 2018.\n",
      "  created infer model with fresh parameters, time 0.16s\n",
      "# Start step 0, lr 0.001, Tue Apr  3 22:12:07 2018\n",
      "# Init train iterator, skipping 0 elements\n",
      "  step 100 lr 0.001 step-time 6.51s wps 0.71K ppl 566.69 gN 15.29 bleu 0.00, Tue Apr  3 22:22:58 2018\n",
      "  step 200 lr 0.001 step-time 6.16s wps 0.75K ppl 331.96 gN 8.45 bleu 0.00, Tue Apr  3 22:33:14 2018\n",
      "  step 300 lr 0.001 step-time 24.76s wps 0.19K ppl 284.91 gN 6.82 bleu 0.00, Tue Apr  3 23:14:30 2018\n",
      "  step 400 lr 0.001 step-time 6.26s wps 0.73K ppl 252.99 gN 6.57 bleu 0.00, Tue Apr  3 23:24:56 2018\n",
      "  step 500 lr 0.001 step-time 7.69s wps 0.60K ppl 227.95 gN 6.44 bleu 0.00, Tue Apr  3 23:37:46 2018\n",
      "  step 600 lr 0.001 step-time 6.05s wps 0.76K ppl 196.96 gN 6.83 bleu 0.00, Tue Apr  3 23:47:51 2018\n",
      "  step 700 lr 0.001 step-time 7.38s wps 0.63K ppl 184.29 gN 7.18 bleu 0.00, Wed Apr  4 00:00:09 2018\n",
      "  step 800 lr 0.001 step-time 6.33s wps 0.73K ppl 168.50 gN 6.69 bleu 0.00, Wed Apr  4 00:10:42 2018\n",
      "  step 900 lr 0.001 step-time 8.40s wps 0.55K ppl 158.37 gN 6.63 bleu 0.00, Wed Apr  4 00:24:42 2018\n",
      "  step 1000 lr 0.0005 step-time 9.46s wps 0.49K ppl 146.95 gN 5.79 bleu 0.00, Wed Apr  4 00:40:28 2018\n",
      "# Save eval, global step 1000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-1000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-1000, time 0.19s\n",
      "  # 1106\n",
      "    src: Nhưng sau đó điều xảy ra là các cá thể hình thành nên đương nhiên rồi , những mánh khoé trong việc trao đổi thông tin .\n",
      "    ref: But then what happened was the individuals worked out , of course , tricks of communicating .\n",
      "    nmt: But then that &apos;s the that that , are , to , , , &apos;s &apos;s , of &apos;s , . the &apos;s of of . .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-1000\n",
      "  loaded eval model parameters from nmt_model_noatt/translate.ckpt-1000, time 0.21s\n",
      "  eval dev: perplexity 139.30, time 69s, Wed Apr  4 00:41:45 2018.\n",
      "  eval test: perplexity 160.64, time 62s, Wed Apr  4 00:42:47 2018.\n",
      "# Finished an epoch, step 1043. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-1000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-1000, time 0.13s\n",
      "  # 756\n",
      "    src: Bạn biết loại hổ thẹn lành mạnh ấy , khi bạn để lộ một bí mật mà anh bạn thân nhất bắt bạn thề không bao giờ hở ra và rồi bạn bị vạch mặt và anh bạn thân nhất kia đối chất bạn và hai người có những đoạn hội thoại khủng khiếp nhưng cuối cùng thì cảm giác hổ thẹn vạch đường cho bạn và bạn nói , tôi sẽ không bao giờ phạm sai lầm như thế nữa .\n",
      "    ref: You know that healthy kind , when you betray a secret that a best friend made you promise never to reveal and then you get busted and then your best friend confronts you and you have terrible discussions , but at the end of it all that sick feeling guides you and you say , I &apos;ll never make that mistake again .\n",
      "    nmt: You know , the you of , , you you a a of of , , , you you a to of a , , you you , , , you you , , , you you , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-1000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-1000, time 0.09s\n",
      "# External evaluation, global step 1000\n",
      "  decoding to output nmt_model_noatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 111s, Wed Apr  4 00:50:44 2018.\n",
      "  bleu dev: 1.9\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "# External evaluation, global step 1000\n",
      "  decoding to output nmt_model_noatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 179s, Wed Apr  4 00:53:44 2018.\n",
      "  bleu test: 1.1\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "  step 1100 lr 0.00025 step-time 11.19s wps 0.40K ppl 130.61 gN 5.60 bleu 1.85, Wed Apr  4 01:06:20 2018\n",
      "  step 1200 lr 0.000125 step-time 12.56s wps 0.36K ppl 123.98 gN 5.48 bleu 1.85, Wed Apr  4 01:27:16 2018\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-1200\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-1200, time 0.12s\n",
      "  # 1312\n",
      "    src: Tôi có một câu hỏi cho tất cả các bạn ở đây : Khi có những việc quan trọng cần đến sự chung tay của chúng ta , tất cả chúng ta , chúng ta sẽ là một đám đông tiếng nói , hay chúng ta sẽ trở thành một nhóm của những đôi tay ?\n",
      "    ref: So the question I have for all of you here : When it comes to the big , important things that we need to do together , all of us together , are we just going to be a crowd of voices , or are we also going to be a crowd of hands ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    nmt: I I a a of of , , you you to to the the of of , , we we to to this this of of that that , is we we to to , , , , , , , , , , , , , , the the of of , ,\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-1200\n",
      "  loaded eval model parameters from nmt_model_noatt/translate.ckpt-1200, time 0.17s\n",
      "  eval dev: perplexity 133.81, time 72s, Wed Apr  4 01:28:33 2018.\n",
      "  eval test: perplexity 155.38, time 68s, Wed Apr  4 01:29:41 2018.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-1200\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-1200, time 0.17s\n",
      "# External evaluation, global step 1200\n",
      "  decoding to output nmt_model_noatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 175s, Wed Apr  4 01:32:37 2018.\n",
      "  bleu dev: 1.8\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "# External evaluation, global step 1200\n",
      "  decoding to output nmt_model_noatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 158s, Wed Apr  4 01:35:16 2018.\n",
      "  bleu test: 1.2\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "# Final, step 1200 lr 0.000125 step-time 12.56s wps 0.36K ppl 123.98 gN 5.48 dev ppl 133.81, dev bleu 1.8, test ppl 155.38, test bleu 1.2, Wed Apr  4 01:35:17 2018\n",
      "# Done training!, time 12190s, Wed Apr  4 01:35:17 2018.\n",
      "# Start evaluating saved best models.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/best_bleu/translate.ckpt-1000\n",
      "  loaded infer model parameters from nmt_model_noatt/best_bleu/translate.ckpt-1000, time 0.17s\n",
      "  # 1479\n",
      "    src: Vào thập kỷ 80 tại Đông Đức cũ nếu bạn sở hữu một chiếc máy đánh chữ bạn sẽ phải đăng ký nó với chính quyền\n",
      "    ref: In the 1980s in the communist Eastern Germany , if you owned a typewriter , you had to register it with the government .\n",
      "    nmt: In the , , you you a a of of , , you you a a of of , , you you a a of\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/best_bleu/translate.ckpt-1000\n",
      "  loaded eval model parameters from nmt_model_noatt/best_bleu/translate.ckpt-1000, time 0.15s\n",
      "  eval dev: perplexity 139.30, time 64s, Wed Apr  4 01:36:22 2018.\n",
      "  eval test: perplexity 160.64, time 63s, Wed Apr  4 01:37:26 2018.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/best_bleu/translate.ckpt-1000\n",
      "  loaded infer model parameters from nmt_model_noatt/best_bleu/translate.ckpt-1000, time 0.18s\n",
      "# External evaluation, global step 1000\n",
      "  decoding to output nmt_model_noatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 166s, Wed Apr  4 01:40:12 2018.\n",
      "  bleu dev: 1.9\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "# External evaluation, global step 1000\n",
      "  decoding to output nmt_model_noatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 164s, Wed Apr  4 01:42:57 2018.\n",
      "  bleu test: 1.1\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "# Best bleu, step 1000 lr 0.000125 step-time 12.56s wps 0.36K ppl 123.98 gN 5.48 dev ppl 139.30, dev bleu 1.9, test ppl 160.64, test bleu 1.1, Wed Apr  4 01:42:58 2018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'dev_ppl': 133.81430230754515,\n",
       "  'dev_scores': {'bleu': 1.831062802077899},\n",
       "  'test_ppl': 155.38244108151724,\n",
       "  'test_scores': {'bleu': 1.1960515539709407}},\n",
       " 1200)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If desired as a baseline, train a vanilla LSTM model without attention\n",
    "hparams = create_standard_hparams(\n",
    "    data_path=os.path.join(\"datasets\", \"nmt_data_vi\"), \n",
    "    out_dir=\"nmt_model_noatt\"\n",
    ")\n",
    "hparams.add_hparam(\"attention_cell_class\", LSTMCellWithDummyAttention)\n",
    "train(hparams, AttentionalModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Vocab file datasets/nmt_data_vi/vocab.vi exists\n",
      "# Vocab file datasets/nmt_data_vi/vocab.en exists\n",
      "# creating train graph ...\n",
      "  num_layers = 1, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
      "  cell 0512\n",
      "  DropoutWrapper, dropout=0.2   DropoutWrapper  DeviceWrapper, device=/gpu:0\n",
      "  learning_rate=0.001, warmup_steps=0, warmup_scheme=t2t\n",
      "  decay_scheme=luong234, start_decay_step=800, decay_steps 100, decay_factor 0.5\n",
      "# Trainable variables\n",
      "  embeddings/encoder/embedding_encoder:0, (7709, 512), /device:GPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (17191, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_dot_product_attention/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_dot_product_attention/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_dot_product_attention/W_c:0, (1024, 256), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (256, 17191), \n",
      "# creating eval graph ...\n",
      "  num_layers = 1, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 0512\n",
      "  LSTMCellWithDotProductAttention, dropout=0   LSTMCellWithDotProductAttention  DeviceWrapper, device=/gpu:0\n",
      "# Trainable variables\n",
      "  embeddings/encoder/embedding_encoder:0, (7709, 512), /device:GPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (17191, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_dot_product_attention/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_dot_product_attention/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_dot_product_attention/W_c:0, (1024, 256), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (256, 17191), \n",
      "# creating infer graph ...\n",
      "  num_layers = 1, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 0512\n",
      "  LSTMCellWithDotProductAttention, dropout=0   LSTMCellWithDotProductAttention  DeviceWrapper, device=/gpu:0\n",
      "# Trainable variables\n",
      "  embeddings/encoder/embedding_encoder:0, (7709, 512), /device:GPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (17191, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_dot_product_attention/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_dot_product_attention/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_dot_product_attention/W_c:0, (1024, 256), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (256, 17191), \n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-50\n",
      "  loaded train model parameters from nmt_model_dotprodatt/translate.ckpt-50, time 3.49s\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-50\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-50, time 0.59s\n",
      "  # 1134\n",
      "    src: Và trên thực tế , đôi khi chúng thực hiện bởi những phương pháp chúng tôi thậm chí không hiểu rõ lắm .\n",
      "    ref: And in fact , sometimes it takes it by methods that we don &apos;t quite even understand .\n",
      "    nmt: And , , , , , , , , , , , , , , , ,\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-50\n",
      "  loaded eval model parameters from nmt_model_dotprodatt/translate.ckpt-50, time 0.82s\n",
      "  eval dev: perplexity 393.79, time 105s, Wed Apr  4 01:44:59 2018.\n",
      "  eval test: perplexity 448.05, time 121s, Wed Apr  4 01:47:01 2018.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-50\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-50, time 0.13s\n",
      "# External evaluation, global step 50\n",
      "  decoding to output nmt_model_dotprodatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 202s, Wed Apr  4 01:50:23 2018.\n",
      "  bleu dev: 0.0\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "# External evaluation, global step 50\n",
      "  decoding to output nmt_model_dotprodatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 202s, Wed Apr  4 01:53:46 2018.\n",
      "  bleu test: 0.0\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "# Start step 50, lr 0.001, Wed Apr  4 01:53:47 2018\n",
      "# Init train iterator, skipping 0 elements\n",
      "  step 150 lr 0.001 step-time 12.52s wps 0.37K ppl 394.81 gN 13.41 bleu 0.00, Wed Apr  4 02:14:39 2018\n",
      "  step 250 lr 0.001 step-time 11.95s wps 0.38K ppl 333.13 gN 9.91 bleu 0.00, Wed Apr  4 02:34:33 2018\n",
      "  step 350 lr 0.001 step-time 11.91s wps 0.38K ppl 287.81 gN 8.83 bleu 0.00, Wed Apr  4 02:54:24 2018\n",
      "  step 450 lr 0.001 step-time 12.06s wps 0.38K ppl 249.49 gN 8.28 bleu 0.00, Wed Apr  4 03:14:30 2018\n",
      "  step 550 lr 0.001 step-time 12.04s wps 0.38K ppl 218.78 gN 8.63 bleu 0.00, Wed Apr  4 03:34:35 2018\n",
      "  step 650 lr 0.001 step-time 11.92s wps 0.38K ppl 194.22 gN 7.70 bleu 0.00, Wed Apr  4 03:54:26 2018\n",
      "  step 750 lr 0.001 step-time 11.99s wps 0.38K ppl 177.74 gN 7.97 bleu 0.00, Wed Apr  4 04:14:25 2018\n",
      "  step 850 lr 0.001 step-time 12.09s wps 0.38K ppl 167.10 gN 7.89 bleu 0.00, Wed Apr  4 04:34:34 2018\n",
      "  step 950 lr 0.0005 step-time 12.04s wps 0.38K ppl 151.64 gN 6.78 bleu 0.00, Wed Apr  4 04:54:38 2018\n",
      "  step 1050 lr 0.00025 step-time 11.89s wps 0.38K ppl 140.89 gN 6.03 bleu 0.00, Wed Apr  4 05:14:27 2018\n",
      "# Save eval, global step 1050\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-1050\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-1050, time 0.10s\n",
      "  # 256\n",
      "    src: không ngừng gây ra căng thẳng và bất hoà\n",
      "    ref: We &apos;re constantly creating tensions and conflicts .\n",
      "    nmt: It &apos;s &apos;t <unk> <unk> and\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-1050\n",
      "  loaded eval model parameters from nmt_model_dotprodatt/translate.ckpt-1050, time 0.16s\n",
      "  eval dev: perplexity 138.64, time 102s, Wed Apr  4 05:16:15 2018.\n",
      "  eval test: perplexity 160.81, time 120s, Wed Apr  4 05:18:15 2018.\n",
      "# Finished an epoch, step 1093. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-1050\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-1050, time 0.14s\n",
      "  # 202\n",
      "    src: theo tôi , sự phạt tù rộng rãi như thế đã cơ bản thay đổi thế giới này\n",
      "    ref: And mass incarceration , in my judgment , has fundamentally changed our world .\n",
      "    nmt: So , , , was was the the of of that that of of is is to to\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-1050\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-1050, time 0.11s\n",
      "# External evaluation, global step 1050\n",
      "  decoding to output nmt_model_dotprodatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 267s, Wed Apr  4 05:30:44 2018.\n",
      "  bleu dev: 1.4\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "# External evaluation, global step 1050\n",
      "  decoding to output nmt_model_dotprodatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 270s, Wed Apr  4 05:35:16 2018.\n",
      "  bleu test: 0.9\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "  step 1150 lr 0.000125 step-time 11.96s wps 0.38K ppl 130.64 gN 5.95 bleu 1.43, Wed Apr  4 05:47:11 2018\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-1200\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-1200, time 0.14s\n",
      "  # 153\n",
      "    src: tôi nhớ như in ngày đó , tưởng chừng như mới xảy qua hôm qua vậy\n",
      "    ref: And I remember this just like it happened yesterday .\n",
      "    nmt: I I , , , , , the the of of is is in the . .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-1200\n",
      "  loaded eval model parameters from nmt_model_dotprodatt/translate.ckpt-1200, time 0.18s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  eval dev: perplexity 135.87, time 102s, Wed Apr  4 05:59:04 2018.\n",
      "  eval test: perplexity 157.23, time 119s, Wed Apr  4 06:01:03 2018.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-1200\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-1200, time 0.16s\n",
      "# External evaluation, global step 1200\n",
      "  decoding to output nmt_model_dotprodatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 269s, Wed Apr  4 06:05:33 2018.\n",
      "  bleu dev: 1.5\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "# External evaluation, global step 1200\n",
      "  decoding to output nmt_model_dotprodatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 275s, Wed Apr  4 06:10:10 2018.\n",
      "  bleu test: 1.0\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "# Final, step 1200 lr 0.000125 step-time 11.96s wps 0.38K ppl 130.64 gN 5.95 dev ppl 135.87, dev bleu 1.5, test ppl 157.23, test bleu 1.0, Wed Apr  4 06:10:11 2018\n",
      "# Done training!, time 15384s, Wed Apr  4 06:10:11 2018.\n",
      "# Start evaluating saved best models.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/best_bleu/translate.ckpt-1200\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/best_bleu/translate.ckpt-1200, time 0.13s\n",
      "  # 1295\n",
      "    src: Chúng ta sẽ đạt được những điều chúng ta cần phải làm mà không chấn chỉnh lại chính quyền đại diện cho tất cả chúng ta không ?\n",
      "    ref: Do we really think we &apos;re going to get where we need to go without fixing the one institution that can act on behalf of all of us ?\n",
      "    nmt: We we &apos;t to to that that we can &apos;t the to that that of can is be we to to to , ? we\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/best_bleu/translate.ckpt-1200\n",
      "  loaded eval model parameters from nmt_model_dotprodatt/best_bleu/translate.ckpt-1200, time 0.16s\n",
      "  eval dev: perplexity 135.87, time 103s, Wed Apr  4 06:11:55 2018.\n",
      "  eval test: perplexity 157.23, time 120s, Wed Apr  4 06:13:56 2018.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/best_bleu/translate.ckpt-1200\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/best_bleu/translate.ckpt-1200, time 0.15s\n",
      "# External evaluation, global step 1200\n",
      "  decoding to output nmt_model_dotprodatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 269s, Wed Apr  4 06:18:25 2018.\n",
      "  bleu dev: 1.5\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "# External evaluation, global step 1200\n",
      "  decoding to output nmt_model_dotprodatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 276s, Wed Apr  4 06:23:02 2018.\n",
      "  bleu test: 1.0\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "# Best bleu, step 1200 lr 0.000125 step-time 11.96s wps 0.38K ppl 130.64 gN 5.95 dev ppl 135.87, dev bleu 1.5, test ppl 157.23, test bleu 1.0, Wed Apr  4 06:23:03 2018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'dev_ppl': 135.87327272649367,\n",
       "  'dev_scores': {'bleu': 1.4760935377844826},\n",
       "  'test_ppl': 157.2252914920143,\n",
       "  'test_scores': {'bleu': 0.9552532782968963}},\n",
       " 1200)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train an LSTM model with dot-product attention\n",
    "hparams = create_standard_hparams(data_path=os.path.join(\"datasets\", \"nmt_data_vi\"), \n",
    "                                  out_dir=\"nmt_model_dotprodatt\")\n",
    "hparams.add_hparam(\"attention_cell_class\", LSTMCellWithDotProductAttention)\n",
    "train(hparams, AttentionalModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Vocab file datasets/nmt_data_vi/vocab.vi exists\n",
      "# Vocab file datasets/nmt_data_vi/vocab.en exists\n",
      "# creating train graph ...\n",
      "  num_layers = 1, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
      "  cell 0512\n",
      "  DropoutWrapper, dropout=0.2   DropoutWrapper  DeviceWrapper, device=/gpu:0\n",
      "  learning_rate=0.001, warmup_steps=0, warmup_scheme=t2t\n",
      "  decay_scheme=luong234, start_decay_step=800, decay_steps 100, decay_factor 0.5\n",
      "# Trainable variables\n",
      "  embeddings/encoder/embedding_encoder:0, (7709, 512), /device:GPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (17191, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_bilinear_attention/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_bilinear_attention/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_bilinear_attention/W_att:0, (512, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_bilinear_attention/W_c:0, (1024, 256), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (256, 17191), \n",
      "# creating eval graph ...\n",
      "  num_layers = 1, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 0512\n",
      "  LSTMCellWithBilinearAttention, dropout=0   LSTMCellWithBilinearAttention  DeviceWrapper, device=/gpu:0\n",
      "# Trainable variables\n",
      "  embeddings/encoder/embedding_encoder:0, (7709, 512), /device:GPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (17191, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_bilinear_attention/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_bilinear_attention/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_bilinear_attention/W_att:0, (512, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_bilinear_attention/W_c:0, (1024, 256), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (256, 17191), \n",
      "# creating infer graph ...\n",
      "  num_layers = 1, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 0512\n",
      "  LSTMCellWithBilinearAttention, dropout=0   LSTMCellWithBilinearAttention  DeviceWrapper, device=/gpu:0\n",
      "# Trainable variables\n",
      "  embeddings/encoder/embedding_encoder:0, (7709, 512), /device:GPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (17191, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_bilinear_attention/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_bilinear_attention/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_bilinear_attention/W_att:0, (512, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_bilinear_attention/W_c:0, (1024, 256), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (256, 17191), \n",
      "  created train model with fresh parameters, time 3.35s\n",
      "  created infer model with fresh parameters, time 0.65s\n",
      "  # 1458\n",
      "    src: Ý tưởng đằng sau littleBits là xem nó như thể một dạng thư viện đang được phát triển .\n",
      "    ref: The idea behind littleBits is that it &apos;s a growing library .\n",
      "    nmt: foreigner cartel cartel cartel cartel illegal physics Rainforest boldly 1963 TH grape grape grape grape grape Japanese-Americans Japanese-Americans bankers welfare bathtub welfare welfare welfare welfare tidied tidied digging digging digging digging Instead Instead Instead Instead Instead Instead Instead\n",
      "  created eval model with fresh parameters, time 0.84s\n",
      "  eval dev: perplexity 17351.43, time 104s, Wed Apr  4 06:25:04 2018.\n",
      "  eval test: perplexity 17401.15, time 120s, Wed Apr  4 06:27:05 2018.\n",
      "  created infer model with fresh parameters, time 0.19s\n",
      "# Start step 0, lr 0.001, Wed Apr  4 06:27:05 2018\n",
      "# Init train iterator, skipping 0 elements\n",
      "  step 100 lr 0.001 step-time 12.57s wps 0.37K ppl 596.65 gN 14.51 bleu 0.00, Wed Apr  4 06:48:02 2018\n",
      "  step 200 lr 0.001 step-time 12.17s wps 0.38K ppl 361.77 gN 11.25 bleu 0.00, Wed Apr  4 07:08:19 2018\n",
      "  step 300 lr 0.001 step-time 12.25s wps 0.38K ppl 310.52 gN 9.26 bleu 0.00, Wed Apr  4 07:28:44 2018\n",
      "  step 400 lr 0.001 step-time 12.16s wps 0.38K ppl 268.07 gN 8.62 bleu 0.00, Wed Apr  4 07:49:00 2018\n",
      "  step 500 lr 0.001 step-time 12.50s wps 0.38K ppl 237.20 gN 9.42 bleu 0.00, Wed Apr  4 08:09:50 2018\n",
      "  step 600 lr 0.001 step-time 12.16s wps 0.38K ppl 200.81 gN 7.67 bleu 0.00, Wed Apr  4 08:30:07 2018\n",
      "  step 700 lr 0.001 step-time 12.19s wps 0.38K ppl 182.56 gN 8.62 bleu 0.00, Wed Apr  4 08:50:25 2018\n",
      "  step 800 lr 0.001 step-time 12.18s wps 0.38K ppl 166.77 gN 7.26 bleu 0.00, Wed Apr  4 09:10:43 2018\n",
      "  step 900 lr 0.001 step-time 12.60s wps 0.37K ppl 156.78 gN 7.37 bleu 0.00, Wed Apr  4 09:31:43 2018\n",
      "  step 1000 lr 0.0005 step-time 10.99s wps 0.42K ppl 141.33 gN 6.32 bleu 0.00, Wed Apr  4 09:50:02 2018\n",
      "# Save eval, global step 1000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-1000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-1000, time 0.10s\n",
      "  # 670\n",
      "    src: Tốt , cực tốt , có thể là siêu sao .\n",
      "    ref: Good , really good , maybe an all-star .\n",
      "    nmt: Well , , , , , , , , . .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-1000\n",
      "  loaded eval model parameters from nmt_model_bilinearatt/translate.ckpt-1000, time 0.15s\n",
      "  eval dev: perplexity 138.49, time 64s, Wed Apr  4 09:51:09 2018.\n",
      "  eval test: perplexity 161.92, time 74s, Wed Apr  4 09:52:24 2018.\n",
      "# Finished an epoch, step 1043. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-1000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-1000, time 0.15s\n",
      "  # 151\n",
      "    src: thế rồi tôi nhìn bà và cười nhưng bà có vẻ rất nghiêm trọng .\n",
      "    ref: And I would look at her and I &apos;d smile , but she was very serious .\n",
      "    nmt: And I I to and and and , , I I it it .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-1000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-1000, time 0.13s\n",
      "# External evaluation, global step 1000\n",
      "  decoding to output nmt_model_bilinearatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 314s, Wed Apr  4 10:04:14 2018.\n",
      "  bleu dev: 1.6\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "# External evaluation, global step 1000\n",
      "  decoding to output nmt_model_bilinearatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 229s, Wed Apr  4 10:08:07 2018.\n",
      "  bleu test: 1.0\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "  step 1100 lr 0.00025 step-time 14.68s wps 0.30K ppl 126.92 gN 5.91 bleu 1.55, Wed Apr  4 10:26:01 2018\n",
      "  step 1200 lr 0.000125 step-time 20.89s wps 0.22K ppl 120.01 gN 5.92 bleu 1.55, Wed Apr  4 11:00:50 2018\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-1200\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-1200, time 0.31s\n",
      "  # 886\n",
      "    src: À , nó là một ngày thứ 6 , đúng thế .\n",
      "    ref: Well it was a Friday , this is true .\n",
      "    nmt: Well , it &apos;s a a , , , &apos;s &apos;s . .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-1200\n",
      "  loaded eval model parameters from nmt_model_bilinearatt/translate.ckpt-1200, time 0.36s\n",
      "  eval dev: perplexity 133.19, time 162s, Wed Apr  4 11:03:44 2018.\n",
      "  eval test: perplexity 155.69, time 180s, Wed Apr  4 11:06:45 2018.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-1200\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-1200, time 0.20s\n",
      "# External evaluation, global step 1200\n",
      "  decoding to output nmt_model_bilinearatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 397s, Wed Apr  4 11:13:22 2018.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bleu dev: 1.6\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "# External evaluation, global step 1200\n",
      "  decoding to output nmt_model_bilinearatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 409s, Wed Apr  4 11:20:15 2018.\n",
      "  bleu test: 1.1\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "# Final, step 1200 lr 0.000125 step-time 20.89s wps 0.22K ppl 120.01 gN 5.92 dev ppl 133.19, dev bleu 1.6, test ppl 155.69, test bleu 1.1, Wed Apr  4 11:20:17 2018\n",
      "# Done training!, time 17591s, Wed Apr  4 11:20:17 2018.\n",
      "# Start evaluating saved best models.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/best_bleu/translate.ckpt-1200\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/best_bleu/translate.ckpt-1200, time 0.16s\n",
      "  # 308\n",
      "    src: cố gắng đương đầu với sự thiên vị và phân biệt xử trong cách xét xử tư pháp ,\n",
      "    ref: We &apos;re trying to confront bias and discrimination in the administration of criminal justice .\n",
      "    nmt: What to to to the the of of , is the the of of of and and , the the of of .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/best_bleu/translate.ckpt-1200\n",
      "  loaded eval model parameters from nmt_model_bilinearatt/best_bleu/translate.ckpt-1200, time 0.23s\n",
      "  eval dev: perplexity 133.19, time 142s, Wed Apr  4 11:22:40 2018.\n",
      "  eval test: perplexity 155.69, time 141s, Wed Apr  4 11:25:01 2018.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/best_bleu/translate.ckpt-1200\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/best_bleu/translate.ckpt-1200, time 0.19s\n",
      "# External evaluation, global step 1200\n",
      "  decoding to output nmt_model_bilinearatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 284s, Wed Apr  4 11:29:46 2018.\n",
      "  bleu dev: 1.6\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "# External evaluation, global step 1200\n",
      "  decoding to output nmt_model_bilinearatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 284s, Wed Apr  4 11:34:31 2018.\n",
      "  bleu test: 1.1\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "# Best bleu, step 1200 lr 0.000125 step-time 20.89s wps 0.22K ppl 120.01 gN 5.92 dev ppl 133.19, dev bleu 1.6, test ppl 155.69, test bleu 1.1, Wed Apr  4 11:34:32 2018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'dev_ppl': 133.18707259164182,\n",
       "  'dev_scores': {'bleu': 1.5708186258248336},\n",
       "  'test_ppl': 155.68895230529802,\n",
       "  'test_scores': {'bleu': 1.0728379135015003}},\n",
       " 1200)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train an LSTM model with bilinear attention\n",
    "hparams = create_standard_hparams(data_path=os.path.join(\"datasets\", \"nmt_data_vi\"),\n",
    "                                  out_dir=\"nmt_model_bilinearatt\")\n",
    "hparams.add_hparam(\"attention_cell_class\", LSTMCellWithBilinearAttention)\n",
    "train(hparams, AttentionalModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Vocab file datasets/nmt_data_vi/vocab.vi exists\n",
      "# Vocab file datasets/nmt_data_vi/vocab.en exists\n",
      "# creating train graph ...\n",
      "  num_layers = 1, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
      "  cell 0512\n",
      "  DropoutWrapper, dropout=0.2   DropoutWrapper  DeviceWrapper, device=/gpu:0\n",
      "  learning_rate=0.001, warmup_steps=0, warmup_scheme=t2t\n",
      "  decay_scheme=luong234, start_decay_step=800, decay_steps 100, decay_factor 0.5\n",
      "# Trainable variables\n",
      "  embeddings/encoder/embedding_encoder:0, (7709, 512), /device:GPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (17191, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_feed_forward_attention/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_feed_forward_attention/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_feed_forward_attention/W_att_1:0, (1024, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_feed_forward_attention/W_att_2:0, (512, 1), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_feed_forward_attention/W_c:0, (1024, 256), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (256, 17191), \n",
      "# creating eval graph ...\n",
      "  num_layers = 1, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 0512\n",
      "  LSTMCellWithFeedForwardAttention, dropout=0   LSTMCellWithFeedForwardAttention  DeviceWrapper, device=/gpu:0\n",
      "# Trainable variables\n",
      "  embeddings/encoder/embedding_encoder:0, (7709, 512), /device:GPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (17191, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_feed_forward_attention/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_feed_forward_attention/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_feed_forward_attention/W_att_1:0, (1024, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_feed_forward_attention/W_att_2:0, (512, 1), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_feed_forward_attention/W_c:0, (1024, 256), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (256, 17191), \n",
      "# creating infer graph ...\n",
      "  num_layers = 1, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 0512\n",
      "  LSTMCellWithFeedForwardAttention, dropout=0   LSTMCellWithFeedForwardAttention  DeviceWrapper, device=/gpu:0\n",
      "# Trainable variables\n",
      "  embeddings/encoder/embedding_encoder:0, (7709, 512), /device:GPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (17191, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_feed_forward_attention/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_feed_forward_attention/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_feed_forward_attention/W_att_1:0, (1024, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_feed_forward_attention/W_att_2:0, (512, 1), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_feed_forward_attention/W_c:0, (1024, 256), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (256, 17191), \n",
      "  created train model with fresh parameters, time 4.12s\n",
      "  created infer model with fresh parameters, time 0.82s\n",
      "  # 762\n",
      "    src: Và không phải vì bác sĩ hướng dẫn của tôi ; anh ấy dễ mến vô cùng .\n",
      "    ref: And it wasn &apos;t because of my attending ; he was a doll .\n",
      "    nmt: exist inscribed queues disruptive especially varied anecdote withheld withheld narratives narratives narratives Fuller Fuller Fuller quicker quicker quicker quicker baseline baseline author author author basement basement weeds weeds bad bad bad bad bad bad bad bad\n",
      "  created eval model with fresh parameters, time 1.01s\n",
      "  eval dev: perplexity 17321.76, time 209s, Wed Apr  4 12:21:56 2018.\n",
      "  eval test: perplexity 17355.26, time 230s, Wed Apr  4 12:25:47 2018.\n",
      "  created infer model with fresh parameters, time 0.30s\n",
      "# Start step 0, lr 0.001, Wed Apr  4 12:25:47 2018\n",
      "# Init train iterator, skipping 0 elements\n",
      "  step 100 lr 0.001 step-time 107.25s wps 0.04K ppl 608.14 gN 16.03 bleu 0.00, Wed Apr  4 15:24:33 2018\n"
     ]
    }
   ],
   "source": [
    "# Train an LSTM model with feedforward attention\n",
    "hparams = create_standard_hparams(data_path=os.path.join(\"datasets\", \"nmt_data_vi\"), \n",
    "                                  out_dir=\"nmt_model_ffatt\")\n",
    "hparams.add_hparam(\"attention_cell_class\", LSTMCellWithFeedForwardAttention)\n",
    "train(hparams, AttentionalModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
